function [y1] = NN_QAM_3dB(x1)
% This neural network is trained at 3dB to detect the order of QAM
% modulations
% Generated by Neural Network Toolbox function genFunction, 30-Nov-2016 01:35:19.
%
% [y1] = myNeuralNetworkFunction(x1) takes these arguments:
%   x = 13xQ matrix, input #1
% and returns:
%   y = 3xQ matrix, output #1
% where Q is the number of samples.

%#ok<*RPMT0>

% ===== NEURAL NETWORK CONSTANTS =====

% Input 1
x1_step1_xoffset = [-0.772567579917793;-0.0990725707093899;1.32508770445967;-1.04427603448341;-0.748098449783081;-0.925555275432344;-1.98423927492414;-7.47147193936438;2.08884201116147;-1.75757233694278;0.287775560558287;-1.42087507838754;1.1383811884297];
x1_step1_gain = [1.73150547256041;0.537304639343263;0.183058206352172;2.90274983607034;2.22846058165612;0.19163420620895;0.53248971951076;0.191937190803046;0.0241046130529755;0.37712157752176;0.0835591732973527;0.0445164081726322;0.823841576330237];
x1_step1_ymin = -1;

% Layer 1
b1 = [-1.039697832655252;-5.3698701810871521;1.2955913926785165];
IW1_1 = [3.155639815628795 1.4793549812784523 0.21908667222940859 -1.692896226073308 -3.6528463565325593 -0.97308815903293933 -0.98908667188038535 1.7053039018601219 -0.57861078212789441 1.9977041308946706 0.91004314908480033 0.37541237476132311 1.2421655643424441;7.5340605955969115 5.3924541707972597 -1.6719196819693114 -2.2724374485907699 -1.5329747415402819 -2.5906016218728478 -1.1615502910290934 -4.6428594222951327 2.0906395528744266 -0.47944032178085977 -23.487776043941818 3.7550837991929535 4.1907210833439761;3.3895849316134603 1.8872484529125673 -0.29536538343147628 -1.2596092981219877 -3.0017156168512553 0.295883513981194 -0.56186102788120018 0.47137355182751089 -0.21961282663174403 1.1944954958395217 1.2181512039174374 1.3365663462070008 2.4536771046734742];

% Layer 2
b2 = [0.49221161203655223;-2.0259727842295181;-0.84366130198883671];
LW2_1 = [11.546344659748431 10.429942843593732 11.71107472109588;-5.354005394603667 0.32478955540298637 -5.9149222431610387;-5.3699325685782027 -10.348489990866696 -7.0060648735277429];

% ===== SIMULATION ========

% Dimensions
Q = size(x1,2); % samples

% Input 1
xp1 = mapminmax_apply(x1,x1_step1_gain,x1_step1_xoffset,x1_step1_ymin);

% Layer 1
a1 = tansig_apply(repmat(b1,1,Q) + IW1_1*xp1);

% Layer 2
a2 = softmax_apply(repmat(b2,1,Q) + LW2_1*a1);

% Output 1
y1 = a2;
end

% ===== MODULE FUNCTIONS ========

% Map Minimum and Maximum Input Processing Function
function y = mapminmax_apply(x,settings_gain,settings_xoffset,settings_ymin)
y = bsxfun(@minus,x,settings_xoffset);
y = bsxfun(@times,y,settings_gain);
y = bsxfun(@plus,y,settings_ymin);
end

% Competitive Soft Transfer Function
function a = softmax_apply(n)
nmax = max(n,[],1);
n = bsxfun(@minus,n,nmax);
numer = exp(n);
denom = sum(numer,1);
denom(denom == 0) = 1;
a = bsxfun(@rdivide,numer,denom);
end

% Sigmoid Symmetric Transfer Function
function a = tansig_apply(n)
a = 2 ./ (1 + exp(-2*n)) - 1;
end
